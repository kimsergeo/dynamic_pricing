{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:32.688128Z",
     "start_time": "2025-09-01T05:15:32.680129Z"
    }
   },
   "source": [
    "from IPython.display import Image, Math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Импортируем библиотеки для визуализаци данных\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(\"model\")\n",
    "logger.setLevel(logging.INFO)"
   ],
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "## TASK 1"
   ],
   "id": "f47ebb757a45da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:32.750321Z",
     "start_time": "2025-09-01T05:15:32.694177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import data\n",
    "sales_df = pd.read_parquet(\"./data/hw/sales.parquet\")\n",
    "\n",
    "# Assign corect data types\n",
    "sales_df[\"ds\"] = sales_df[\"ds\"].astype(str)"
   ],
   "id": "a5b82c8bf7141e20",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:32.781730Z",
     "start_time": "2025-09-01T05:15:32.754326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the latest prices for each SKU\n",
    "prices_df = sales_df \\\n",
    "    .sort_values([\"ds\"], ascending=False) \\\n",
    "    .groupby([\"sku_id\"], as_index=False) \\\n",
    "    .head(1) \\\n",
    "    .filter([\"sku_id\", \"price\"])\n",
    "\n",
    "# Get unique combinations of dates and groups (categories)\n",
    "ds_df = pd.DataFrame({\"ds\": [\"20240101\", \"20240102\", \"20240103\", \"20240104\", \"20240105\", \"20240106\", \"20240107\"]})\n",
    "group_df = sales_df[[\"sku_id\"]].drop_duplicates()\n",
    "\n",
    "algo_sample = ds_df \\\n",
    "    .join(group_df,\n",
    "          how=\"cross\") \\\n",
    "    .merge(prices_df,\n",
    "           how=\"left\",\n",
    "           left_on=\"sku_id\",\n",
    "           right_on=\"sku_id\")\n",
    "\n",
    "# Concatenate with the main DF\n",
    "sales_df = pd.concat([sales_df, algo_sample])"
   ],
   "id": "ed414a6bffed727f",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:32.797724Z",
     "start_time": "2025-09-01T05:15:32.785731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Dict, Any, Callable, Optional, Callable\n",
    "\n",
    "# Функция для предсказания базового спроса\n",
    "def predict_base_demand(\n",
    "    df: pd.DataFrame,\n",
    "    W: int,\n",
    ") -> List[float]:\n",
    "    preds = df[f\"rolling_quantity_w_{W}\"]\n",
    "    return preds"
   ],
   "id": "31f7e6bf784a6c5b",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:32.813724Z",
     "start_time": "2025-09-01T05:15:32.803726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from typing import List, Tuple\n",
    "# Добавляем новые фичи (окна, сезонные фичи)\n",
    "\n",
    "\n",
    "# Расчет оконных функций\n",
    "def calc_rolling_window(\n",
    "    df: pd.DataFrame,\n",
    "    window: int,\n",
    "    col: str,\n",
    "    lvl: str,\n",
    "    shift: int,\n",
    ") -> pd.DataFrame:\n",
    "    df = df.sort_values([\"ds\", lvl])\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"ds\"].astype(str))\n",
    "    df[\"col\"] = df[col]\n",
    "    rolling_df = df.set_index(\"ts\")\n",
    "    rolling_df = (\n",
    "        rolling_df.groupby([lvl], group_keys=True)[\"col\"]\n",
    "        # используем shift, так как потом нам нужно предсказывать на N дней вперед:\n",
    "        # для однородности датасета используем сдвиг на кол-во дат в предсказании\n",
    "        .apply(lambda x: x.asfreq(\"1D\").rolling(window=window, closed=\"left\", min_periods=0).mean().shift(shift))\n",
    "        .reset_index()\n",
    "        .rename(columns={\"col\": f\"rolling_{col}_w_{window}\"})\n",
    "    )\n",
    "    df = df.merge(rolling_df, how=\"left\", on=[lvl, \"ts\"])\n",
    "    df = df.drop(columns=[\"ts\", \"col\"])\n",
    "    return df\n",
    "    \n",
    "# Функция для преобразования данных\n",
    "def postprocess_transform(\n",
    "    df: pd.DataFrame,\n",
    "    norms: List[Tuple[str, str]],\n",
    "    roll_cols: List[str],\n",
    "    windows: List[int],\n",
    "    dropna_cols: List[str],\n",
    "    lvl: str,\n",
    "    shift: int,\n",
    "):\n",
    "    # new features\n",
    "    for window in windows:\n",
    "        for col in roll_cols:\n",
    "            logger.info(f\"Rolling window={window} days for col `{col}`\")\n",
    "            df = calc_rolling_window(df=df, window=window, col=col, lvl=lvl, shift=shift)\n",
    "\n",
    "    # normalisation\n",
    "    # нормализация даст сигнал модели об изменении признаков: если изменилась цена, то к какому изменению спроса это привело?\n",
    "    for col1, col2 in norms:\n",
    "        logger.info(f\"Normalizing `{col1}` / `{col2}`\")\n",
    "        df[col1] = df[col1] / df[col2]\n",
    "\n",
    "    # postprocessing\n",
    "    # для однородности данных удаляем первые даты, по которым собирались окна не по полным данным\n",
    "    disadvantaged_ds_list = sorted(df[\"ds\"].unique())[:max(shift, max(windows))]\n",
    "    df = df[~df[\"ds\"].isin(disadvantaged_ds_list)]\n",
    "\n",
    "    df = df.dropna(subset=dropna_cols)\n",
    "    df = df.round(2)\n",
    "    df = df.sort_values([\"ds\", lvl])\n",
    "    return df\n",
    "\n",
    "# Функция для подсчета дуговой эластичности\n",
    "def calc_elasticity(df: pd.DataFrame, lvl: str) -> pd.DataFrame:\n",
    "    # сортируем для взятия предыдущих значений\n",
    "    df = df.sort_values([\"ds\", lvl])\n",
    "    # считаем дуговую эластичность\n",
    "    df[\"prev_orders_num\"] = df.groupby(lvl)[\"orders_num\"].shift(1)\n",
    "    df[\"prev_price\"] = df.groupby(lvl)[\"price\"].shift(1)\n",
    "    df[\"elasticity\"] = (\n",
    "        ((df[\"orders_num\"] - df[\"prev_orders_num\"]) / (df[\"price\"] - df[\"prev_price\"]))\n",
    "        * ((df[\"price\"] + df[\"prev_price\"]) / (df[\"orders_num\"] + df[\"prev_orders_num\"]))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# Функция для преобразования данных с параметрами для удобства короткого вызова\n",
    "def create_features(df: pd.DataFrame, dropna_cols: List[str], lvl: str) -> pd.DataFrame:\n",
    "    df[\"day_of_week\"] = pd.DatetimeIndex(df[\"ds\"]).day_of_week\n",
    "    df = calc_elasticity(df=df, lvl=lvl)\n",
    "    df = postprocess_transform(\n",
    "        df=df,\n",
    "        norms=[],\n",
    "        roll_cols=[\"orders_num\", \"elasticity\", \"price\"],\n",
    "        dropna_cols=dropna_cols,\n",
    "        windows=[14],\n",
    "        lvl=lvl,\n",
    "        shift=7,\n",
    "    )\n",
    "    return df"
   ],
   "id": "913b95f726e1d40b",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:32.829724Z",
     "start_time": "2025-09-01T05:15:32.817725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# df = create_features(df=sales_df, dropna_cols=[\"orders_num\", \"elasticity\", \"price\"], lvl=\"sku_id\")\n",
    "# df.head()"
   ],
   "id": "78a6d6ee000f5c52",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.081909Z",
     "start_time": "2025-09-01T05:15:32.833725Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get rolling window average for ORDERS_NUM and GMV\n",
    "df_01 = calc_rolling_window(df=sales_df,\n",
    "                            window=14,\n",
    "                            col=\"orders_num\",\n",
    "                            lvl=\"sku_id\",\n",
    "                            shift=7)\n",
    "\n",
    "df_01 = calc_rolling_window(df = df_01,\n",
    "                            window=14,\n",
    "                            col=\"gmv\",\n",
    "                            lvl=\"sku_id\",\n",
    "                            shift=7)"
   ],
   "id": "dd451be7426acf0",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.112609Z",
     "start_time": "2025-09-01T05:15:33.085914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get predictions in necessary format for checker\n",
    "df_01 = df_01 \\\n",
    "    .query(f\"(ds >= '20240101') & (ds <= '20240107')\") \\\n",
    "    .filter([\"sku_id\", \"ds\", \"rolling_gmv_w_14\", \"rolling_orders_num_w_14\"]) \\\n",
    "    .rename(columns={\"rolling_orders_num_w_14\": \"orders_num\",\n",
    "                     \"rolling_gmv_w_14\": \"gmv\"}) \\\n",
    "    .sort_values([\"ds\", \"sku_id\"])\n",
    "\n",
    "# Save the results\n",
    "df_01.to_csv(\"./data/hw/homework_5_1_1.csv\", index=False)"
   ],
   "id": "1207d3f4214f1a92",
   "outputs": [],
   "execution_count": 66
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "## TASK 2"
   ],
   "id": "431bb1df2f83f006"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.127884Z",
     "start_time": "2025-09-01T05:15:33.117624Z"
    }
   },
   "cell_type": "code",
   "source": "lvl=\"sku_id\"",
   "id": "dcbdbe3ca81c624e",
   "outputs": [],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.159850Z",
     "start_time": "2025-09-01T05:15:33.131886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sort the values before shifting\n",
    "elasticity_df = sales_df \\\n",
    "    .query(f\"gmv.notna()\") \\\n",
    "    .sort_values([\"ds\", lvl])\n",
    "\n",
    "# Shift the values by 1 day\n",
    "elasticity_df[\"prev_gmv\"] = elasticity_df.groupby(lvl)[\"gmv\"].shift(1)\n",
    "elasticity_df[\"prev_orders_num\"] = elasticity_df.groupby(lvl)[\"orders_num\"].shift(1)\n",
    "elasticity_df[\"prev_price\"] = elasticity_df.groupby(lvl)[\"price\"].shift(1)\n",
    "\n",
    "# Calculate GMV elasticity\n",
    "elasticity_df[\"elasticity_gmv\"] = (\n",
    "        ((elasticity_df[\"gmv\"] - elasticity_df[\"prev_gmv\"]) / (elasticity_df[\"price\"] - elasticity_df[\"prev_price\"]))\n",
    "        * ((elasticity_df[\"price\"] + elasticity_df[\"prev_price\"]) / (elasticity_df[\"gmv\"] + elasticity_df[\"prev_gmv\"]))\n",
    "    )\n",
    "\n",
    "# Calculate ORDERS_NUM elasticity\n",
    "elasticity_df[\"elasticity_orders_num\"] = (\n",
    "        ((elasticity_df[\"orders_num\"] - elasticity_df[\"prev_orders_num\"]) / (elasticity_df[\"price\"] - elasticity_df[\"prev_price\"]))\n",
    "        * ((elasticity_df[\"price\"] + elasticity_df[\"prev_price\"]) / (elasticity_df[\"orders_num\"] + elasticity_df[\"prev_orders_num\"]))\n",
    "    )\n",
    "\n",
    "# Filter out cases where previous price is the same as current\n",
    "elasticity_df = elasticity_df \\\n",
    "    .query(f\"price != prev_price\")"
   ],
   "id": "41b64ddf5990261",
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.191555Z",
     "start_time": "2025-09-01T05:15:33.163853Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter out cases where elasticity is not within the range\n",
    "elasticity_gmv = elasticity_df \\\n",
    "    .groupby([\"sku_id\"], as_index=False) \\\n",
    "    .agg(elasticity_gmv = (\"elasticity_gmv\", \"mean\"))\n",
    "elasticity_gmv[\"elasticity_gmv\"] = elasticity_gmv[\"elasticity_gmv\"].clip(lower=-3.00, upper=-0.01)\n",
    "\n",
    "elasticity_orders_num = elasticity_df \\\n",
    "    .groupby([\"sku_id\"], as_index=False) \\\n",
    "    .agg(elasticity_orders_num = (\"elasticity_orders_num\", \"mean\"))\n",
    "elasticity_orders_num[\"elasticity_orders_num\"] = elasticity_orders_num[\"elasticity_orders_num\"].clip(lower=-3.00, upper=-0.01)\n",
    "\n",
    "elasticity_df = elasticity_df \\\n",
    "    .filter([\"sku_id\"]) \\\n",
    "    .drop_duplicates() \\\n",
    "    .merge(elasticity_gmv,\n",
    "           how=\"left\",\n",
    "           left_on=\"sku_id\",\n",
    "           right_on=\"sku_id\") \\\n",
    "    .merge(elasticity_orders_num,\n",
    "           how=\"left\",\n",
    "           left_on=\"sku_id\",\n",
    "           right_on=\"sku_id\")\n",
    "\n",
    "# Save the results\n",
    "elasticity_df.to_csv(\"./data/hw/homework_5_1_2.csv\", index=False)"
   ],
   "id": "f38fd587cd94f024",
   "outputs": [],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "## TASK 3"
   ],
   "id": "a0c8872ab9479921"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.206692Z",
     "start_time": "2025-09-01T05:15:33.196460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1 - взял за основу sales.parquet\n",
    "# 2 - для периода предсказаний (20240101 - 20240107) взял последнюю доступную по дате цену на уровне \"sku_id\"\n",
    "# 3 - подсчитал rolling window average по цене со следующими параметрами: lvl=\"sku_id\", window=14, shift=7\n",
    "# 4 - сгенерировал сетку из \"ds\" (20240101 - 20240107) и \"discount\", заджойнил данные выше\n",
    "# 5 - заджойнил базовый спрос (\"gmv\", \"orders_num\") из homework_5_1_1.csv\n",
    "# 6 - заджойнил эластичности из homework_5_1_2.csv\n",
    "# 7 - посчитал предсказания \"gmv\" и \"orders_num\"\n",
    "# 8 - посчитал \"margin\"\n",
    "# 9 - сохранил результат и отправил в LMS"
   ],
   "id": "2107c1871ca9c47b",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.348302Z",
     "start_time": "2025-09-01T05:15:33.213700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rolling_price_df = calc_rolling_window(df=sales_df,\n",
    "                                       window=14,\n",
    "                                       col=\"price\",\n",
    "                                       lvl=\"sku_id\",\n",
    "                                       shift=7)"
   ],
   "id": "edd94e0257204b27",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.364279Z",
     "start_time": "2025-09-01T05:15:33.353280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction_df = rolling_price_df \\\n",
    "    .query(f\"'20240101' <= ds <= '20240107'\") \\\n",
    "    .filter([\"sku_id\", \"ds\", \"price\", \"rolling_price_w_14\"]) \\\n",
    "    .merge(df_01,\n",
    "           how=\"left\",\n",
    "           on=[\"sku_id\", \"ds\"]) \\\n",
    "    .merge(elasticity_df,\n",
    "           how=\"left\",\n",
    "           on=\"sku_id\")"
   ],
   "id": "2bd760ab4c4e874d",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.380279Z",
     "start_time": "2025-09-01T05:15:33.368281Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get unique combinations of dates and groups (categories)\n",
    "ds_df = pd.DataFrame({\"ds\": [\"20240101\", \"20240102\", \"20240103\", \"20240104\", \"20240105\", \"20240106\", \"20240107\"]})\n",
    "group_df = pd.DataFrame({\"discount\": [-0.1, -0.08, -0.06, -0.04, -0.02, 0.0, 0.02, 0.04, 0.06, 0.08, 0.1]})\n",
    "\n",
    "discount_df = ds_df \\\n",
    "    .join(group_df,\n",
    "          how=\"cross\") \\\n",
    "    .reset_index(drop=True)"
   ],
   "id": "5d9cc09109eb7f2f",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.395846Z",
     "start_time": "2025-09-01T05:15:33.384280Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction_df = discount_df \\\n",
    "    .merge(prediction_df,\n",
    "           how=\"left\",\n",
    "           on=\"ds\")"
   ],
   "id": "1c6daf1b3dc1bf8e",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.456947Z",
     "start_time": "2025-09-01T05:15:33.399849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prediction_df = prediction_df \\\n",
    "    .assign(price = lambda x: x[\"price\"] + x[\"price\"] * x[\"discount\"]) \\\n",
    "    .assign(orders_num = lambda x: x[\"orders_num\"] * np.power(x[\"price\"] / x[\"rolling_price_w_14\"], x[\"elasticity_orders_num\"]),\n",
    "            gmv = lambda x: x[\"gmv\"] * np.power(x[\"price\"] / x[\"rolling_price_w_14\"], x[\"elasticity_gmv\"])) \\\n",
    "    .assign(margin = lambda x: x[\"gmv\"] * 0.1 + x[\"gmv\"] * x[\"discount\"]) \\\n",
    "    .filter([\"sku_id\", \"ds\", \"discount\", \"orders_num\", \"gmv\", \"margin\"])\n",
    "\n",
    "# Save the results\n",
    "prediction_df.to_csv(\"./data/hw/homework_5_2.csv\", index=False)"
   ],
   "id": "180d27258e0c125e",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<br><br>\n",
    "## TASK 4"
   ],
   "id": "552dc7311c362dfb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.488785Z",
     "start_time": "2025-09-01T05:15:33.460910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "def log_uplifts(\n",
    "    constraints: Dict[str, float],\n",
    "    maximized_column: str,\n",
    "    optimal_statistics: Dict[str, float],\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Функция для логирования значений метрик и их аплифтов (улучшений).\n",
    "\n",
    "    :param constraints: Словарь ограничений для метрик.\n",
    "    :param maximized_column: Название столбца, который подлежит максимизации.\n",
    "    :param optimal_statistics: Словарь с оптимальными статистическими данными.\n",
    "    \"\"\"\n",
    "    # Логируем значение метрики, которую мы максимизируем\n",
    "    logger.info(f\"Metric: {maximized_column}\", extra={\"value\": optimal_statistics.get(maximized_column)})\n",
    "\n",
    "    # Проходим по всем метрикам и их ограничениям\n",
    "    for metric, constraint in constraints.items():\n",
    "        optimal_value = optimal_statistics.get(metric)\n",
    "        if optimal_value is None:\n",
    "            raise ValueError(f\"`{metric}` has not been counted\")\n",
    "        # Логируем информацию по каждой метрике, включая аплифты\n",
    "        log_dict = {\n",
    "            \"constraint value\": round(constraint, 3),\n",
    "            \"optimal value\": round(optimal_value, 3),\n",
    "            \"uplift (abs)\": round(optimal_value - constraint, 3),\n",
    "            \"uplift (pct)\": round(optimal_value * 100 / constraint - 100, 3),\n",
    "        }\n",
    "        logger.info(f\"Metric: {metric}\")\n",
    "        for key, value in log_dict.items():\n",
    "            logger.info(f\"{key}= {value}\")\n",
    "\n",
    "\n",
    "def apply_constraints(\n",
    "    df: pd.DataFrame,\n",
    "    constraints: Dict[str, float],\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Фильтруем датасет по заданным ограничениям.\n",
    "\n",
    "    :param df: DataFrame с данными для фильтрации.\n",
    "    :param constraints: Словарь ограничений для каждой метрики.\n",
    "    :return: Отфильтрованный DataFrame.\n",
    "    \"\"\"\n",
    "    # Применяем ограничения к датафрейму, фильтруя строки\n",
    "    for metric, constraint in constraints.items():\n",
    "        df = df[df[metric] >= constraint]\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_cum_lambda_metrics(\n",
    "    df: pd.DataFrame,\n",
    "    agg_columns: List[str],\n",
    "    maximized_column: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Считаем агрегированные значения метрик для каждой комбинации лямбда-значений.\n",
    "\n",
    "    :param df: DataFrame с данными.\n",
    "    :param agg_columns: Список столбцов для агрегации.\n",
    "    :param maximized_column: Столбец, который максимизируется.\n",
    "    :return: Агрегированный DataFrame.\n",
    "    \"\"\"\n",
    "    # Группируем данные по комбинации лямбда-значений и агрегируем указанные столбцы\n",
    "    df = df.groupby(\"lambda_combination\").agg({column: \"sum\" for column in agg_columns})\n",
    "    df = df.reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def choose_optimal_values(\n",
    "    metric_lambda_map: Dict[str, float],\n",
    "    df: pd.DataFrame,\n",
    "    levels: List[str],\n",
    "    price_column: str,\n",
    "    maximized_column: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Находим оптимальные цены / наценки для каждого уровня для lambda_value\n",
    "    \"\"\"\n",
    "    # Считаем лагранжианы при lambda_value\n",
    "    df[\"lagrangian\"] = df[maximized_column]\n",
    "    lambda_combination_name = \"\"\n",
    "    for metric, metric_lambda in metric_lambda_map.items():\n",
    "        df[\"lagrangian\"] += df[metric] * metric_lambda\n",
    "        lambda_combination_name += f\"{metric}={metric_lambda}_\"\n",
    "    # Находим максимальный лагранжиан для каждого уровня\n",
    "    optimal_df = df.groupby(levels).agg({\"lagrangian\": \"max\"})\n",
    "    df = df.merge(optimal_df, on=levels + [\"lagrangian\"], how=\"inner\")\n",
    "    # Добавляем колонку с lambda_value для запоминания\n",
    "    df[\"lambda_combination\"] = lambda_combination_name.strip(\"_\")\n",
    "    # Удаляем дубликаты (например, оставляем минимальные цены / наценки из оптимальных),\n",
    "    # так как возможны одни и те же значения метрик для разных цен / наценок\n",
    "    # => одинаковые лагранжианы, а нам нужно выбрать одно значение для каждого уровня\n",
    "    df = df.sort_values(price_column)\n",
    "    df = df.drop_duplicates(subset=levels)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_metric_lambda_maps(lambda_config: Dict[str, Any]) -> List[Dict[str, float]]:\n",
    "    # Получаем список значений для каждого ключа\n",
    "    lambda_lists = list(lambda_config.values())\n",
    "    # Используем meshgrid для генерации всех комбинаций параметров\n",
    "    lambda_mesh = np.meshgrid(*lambda_lists)\n",
    "    # Преобразование в массив и решейпинг\n",
    "    lambda_vars = np.stack(lambda_mesh, axis=-1).reshape(-1, len(lambda_config))\n",
    "    # Создаем список словарей\n",
    "    metric_lambda_maps = [\n",
    "        dict(zip(lambda_config.keys(), combination)) for combination in lambda_vars\n",
    "    ]\n",
    "    return metric_lambda_maps\n",
    "\n",
    "\n",
    "def calculate_lagrangians(\n",
    "    df: pd.DataFrame,\n",
    "    lambda_config: Dict[str, Any],\n",
    "    levels: List[str],\n",
    "    price_column: str,\n",
    "    maximized_column: str,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Для каждого значения lambda находим оптимальные цены / наценки для каждого уровня\n",
    "    \"\"\"\n",
    "    lambda_dfs = []\n",
    "    metric_lambda_maps = get_metric_lambda_maps(lambda_config=lambda_config)\n",
    "    logger.info(\n",
    "        f\"Start calculating lagrangians, {len(metric_lambda_maps)} lambda combinations\"\n",
    "    )\n",
    "    for metric_lambda_map in metric_lambda_maps:\n",
    "        lambda_df = choose_optimal_values(\n",
    "            metric_lambda_map=metric_lambda_map,\n",
    "            df=df,\n",
    "            levels=levels,\n",
    "            price_column=price_column,\n",
    "            maximized_column=maximized_column,\n",
    "        )\n",
    "        lambda_dfs.append(lambda_df)\n",
    "    df = pd.concat(lambda_dfs)\n",
    "    df = df.reset_index(drop=True)\n",
    "    logger.info(f\"Ended calculating lagrangians\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# Общая функция для оптимизации\n",
    "def optimize(\n",
    "    df: pd.DataFrame,\n",
    "    lambda_config: Dict[str, Any],\n",
    "    maximized_column: str,\n",
    "    constraints: Dict[str, float],\n",
    "    levels: List[str],\n",
    "    price_column: str,\n",
    ") -> pd.DataFrame:\n",
    "    logger.info(\"Start choosing optimal prices\")\n",
    "    lambda_df = calculate_lagrangians(\n",
    "        df=df,\n",
    "        lambda_config=lambda_config,\n",
    "        levels=levels,\n",
    "        price_column=price_column,\n",
    "        maximized_column=maximized_column,\n",
    "    )\n",
    "    statistics_df = calculate_cum_lambda_metrics(\n",
    "        df=lambda_df,\n",
    "        agg_columns=[maximized_column] + list(constraints.keys()),\n",
    "        maximized_column=maximized_column,\n",
    "    )\n",
    "    statistics_df = statistics_df.sort_values(maximized_column, ascending=False)\n",
    "    logger.info(f\"\\n{statistics_df.head()}\")\n",
    "    statistics_df = apply_constraints(df=statistics_df, constraints=constraints)\n",
    "    logger.info(f\"\\n{statistics_df.head()}\")\n",
    "    best_lambda = statistics_df[\"lambda_combination\"].tolist()[0]\n",
    "    optimal_statistics = statistics_df[\n",
    "        statistics_df[\"lambda_combination\"] == best_lambda\n",
    "    ].to_dict(orient=\"records\")[0]\n",
    "    optimal_df = lambda_df[lambda_df[\"lambda_combination\"] == best_lambda]\n",
    "    log_uplifts(\n",
    "        constraints=constraints,\n",
    "        maximized_column=maximized_column,\n",
    "        optimal_statistics=optimal_statistics,\n",
    "    )\n",
    "    logger.info(\"Ended choosing optimal prices\")\n",
    "    return optimal_df"
   ],
   "id": "7e7ecd45d8ab4bdb",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.504786Z",
     "start_time": "2025-09-01T05:15:33.495788Z"
    }
   },
   "cell_type": "code",
   "source": "prediction_df",
   "id": "d111471e911ded74",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       sku_id        ds  discount   orders_num           gmv        margin\n",
       "0           1  20240101      -0.1  2363.433516  3.189778e+04      0.000000\n",
       "1           3  20240101      -0.1  1241.987842  1.494497e+04      0.000000\n",
       "2           4  20240101      -0.1  3924.782603  2.281482e+05      0.000000\n",
       "3           7  20240101      -0.1  5218.345346  1.162171e+06      0.000000\n",
       "4           8  20240101      -0.1  5821.836268  6.155718e+05      0.000000\n",
       "...       ...       ...       ...          ...           ...           ...\n",
       "16473     393  20240107       0.1   169.915750  1.733853e+05  34677.065743\n",
       "16474     398  20240107       0.1   301.157897  8.112007e+03   1622.401463\n",
       "16475     399  20240107       0.1    59.358236  2.302252e+03    460.450319\n",
       "16476     400  20240107       0.1    33.588556  1.849073e+03    369.814641\n",
       "16477     401  20240107       0.1  1097.187596  1.214774e+05  24295.485889\n",
       "\n",
       "[16478 rows x 6 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku_id</th>\n",
       "      <th>ds</th>\n",
       "      <th>discount</th>\n",
       "      <th>orders_num</th>\n",
       "      <th>gmv</th>\n",
       "      <th>margin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20240101</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>2363.433516</td>\n",
       "      <td>3.189778e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>20240101</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1241.987842</td>\n",
       "      <td>1.494497e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>20240101</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3924.782603</td>\n",
       "      <td>2.281482e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>20240101</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>5218.345346</td>\n",
       "      <td>1.162171e+06</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>20240101</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>5821.836268</td>\n",
       "      <td>6.155718e+05</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16473</th>\n",
       "      <td>393</td>\n",
       "      <td>20240107</td>\n",
       "      <td>0.1</td>\n",
       "      <td>169.915750</td>\n",
       "      <td>1.733853e+05</td>\n",
       "      <td>34677.065743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16474</th>\n",
       "      <td>398</td>\n",
       "      <td>20240107</td>\n",
       "      <td>0.1</td>\n",
       "      <td>301.157897</td>\n",
       "      <td>8.112007e+03</td>\n",
       "      <td>1622.401463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16475</th>\n",
       "      <td>399</td>\n",
       "      <td>20240107</td>\n",
       "      <td>0.1</td>\n",
       "      <td>59.358236</td>\n",
       "      <td>2.302252e+03</td>\n",
       "      <td>460.450319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16476</th>\n",
       "      <td>400</td>\n",
       "      <td>20240107</td>\n",
       "      <td>0.1</td>\n",
       "      <td>33.588556</td>\n",
       "      <td>1.849073e+03</td>\n",
       "      <td>369.814641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16477</th>\n",
       "      <td>401</td>\n",
       "      <td>20240107</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1097.187596</td>\n",
       "      <td>1.214774e+05</td>\n",
       "      <td>24295.485889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16478 rows × 6 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.568290Z",
     "start_time": "2025-09-01T05:15:33.556291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Считаем ограничения\n",
    "control_margin = prediction_df[prediction_df[\"discount\"] == 0][\"margin\"].sum()\n",
    "control_gmv = prediction_df[prediction_df[\"discount\"] == 0][\"gmv\"].sum()"
   ],
   "id": "54dcc3e813696e20",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:33.646915Z",
     "start_time": "2025-09-01T05:15:33.636916Z"
    }
   },
   "cell_type": "code",
   "source": "np.arange(0.0, 1.1, 0.1).tolist()",
   "id": "27da4129a09f7f8d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6000000000000001,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:34.474498Z",
     "start_time": "2025-09-01T05:15:33.663914Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimal_df = optimize(\n",
    "    df=prediction_df,\n",
    "    # перебираем разные lambda для выручки\n",
    "    lambda_config={\n",
    "        \"margin\": np.arange(0.0, 1.1, 0.1).tolist(),\n",
    "        \"gmv\": np.arange(0.0, 1.1, 0.1).tolist()\n",
    "    },\n",
    "    # указываем, что хотим максимизировать\n",
    "    maximized_column=\"orders_num\",\n",
    "    # указываем ограничения\n",
    "    constraints={\n",
    "        \"margin\": control_margin,\n",
    "        \"gmv\": control_gmv\n",
    "    },\n",
    "    levels=[\"sku_id\", \"ds\"],\n",
    "    price_column=\"discount\",\n",
    ")"
   ],
   "id": "7e54809a356cb754",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Start choosing optimal prices\n",
      "INFO:__main__:Start calculating lagrangians, 121 lambda combinations\n",
      "INFO:__main__:Ended calculating lagrangians\n",
      "INFO:__main__:\n",
      "                   lambda_combination    orders_num  margin           gmv\n",
      "0                  margin=0.0_gmv=0.0  3.357924e+06     0.0  1.976511e+08\n",
      "6   margin=0.0_gmv=0.6000000000000001  3.357924e+06     0.0  1.976511e+08\n",
      "10                 margin=0.0_gmv=1.0  3.357924e+06     0.0  1.976511e+08\n",
      "9                  margin=0.0_gmv=0.9  3.357924e+06     0.0  1.976511e+08\n",
      "8                  margin=0.0_gmv=0.8  3.357924e+06     0.0  1.976511e+08\n",
      "INFO:__main__:\n",
      "                   lambda_combination    orders_num        margin  \\\n",
      "21                 margin=0.1_gmv=1.0  3.309405e+06  3.426072e+07   \n",
      "19                 margin=0.1_gmv=0.8  3.308614e+06  3.426947e+07   \n",
      "20                 margin=0.1_gmv=0.9  3.308614e+06  3.426947e+07   \n",
      "16                 margin=0.1_gmv=0.5  3.308614e+06  3.426947e+07   \n",
      "17  margin=0.1_gmv=0.6000000000000001  3.308614e+06  3.426947e+07   \n",
      "\n",
      "             gmv  \n",
      "21  1.973033e+08  \n",
      "19  1.973032e+08  \n",
      "20  1.973032e+08  \n",
      "16  1.973032e+08  \n",
      "17  1.973032e+08  \n",
      "INFO:__main__:Metric: orders_num\n",
      "INFO:__main__:Metric: margin\n",
      "INFO:__main__:constraint value= 19368542.125\n",
      "INFO:__main__:optimal value= 34260720.599\n",
      "INFO:__main__:uplift (abs)= 14892178.474\n",
      "INFO:__main__:uplift (pct)= 76.888\n",
      "INFO:__main__:Metric: gmv\n",
      "INFO:__main__:constraint value= 193685421.255\n",
      "INFO:__main__:optimal value= 197303312.127\n",
      "INFO:__main__:uplift (abs)= 3617890.873\n",
      "INFO:__main__:uplift (pct)= 1.868\n",
      "INFO:__main__:Ended choosing optimal prices\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T05:15:34.600416Z",
     "start_time": "2025-09-01T05:15:34.588431Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the results\n",
    "optimal_df \\\n",
    "    .filter([\"sku_id\", \"ds\", \"discount\"]) \\\n",
    "    .to_csv(\"./data/hw/homework_5_3.csv\", index=False)"
   ],
   "id": "c8dc6765a7d5a668",
   "outputs": [],
   "execution_count": 81
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
